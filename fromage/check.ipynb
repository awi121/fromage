{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Modified from https://github.com/mlfoundations/open_clip\"\"\"\n",
    "\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import collections\n",
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image, ImageFont\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "# from fromage import utils\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import pdb\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_dataset(args,split, tokenizer, precision: str = 'fp32') -> Dataset:\n",
    "    dataset= CsvDataset(tokenizer, '/home/ubuntu/Project/11-777-Project-aa/Data/WebQA_train_val_final.json',split=split)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "class CsvDataset(Dataset):\n",
    "  def __init__(self, tokenizer,dataset_json_path,use_num_samples=-1,\n",
    "               split=['val'],Qcate=['text'], max_len: int = 32, \n",
    "               sep=\"\\t\", precision: str = 'fp32'):\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "    self.precision = precision\n",
    "\n",
    "    assert os.path.exists(dataset_json_path), \"loader.Dataset: dataset json file doesn't exist! {}\".format(dataset_json_path)\n",
    "    # self.imgid_map=pickle.load(open('/home/ubuntu/Project/11-777-Project-aa/Data/image_id_map_0328.pkl', \"rb\"))\n",
    "\n",
    "    self.instance_list = []\n",
    "    count = 0\n",
    "    with open(dataset_json_path, \"r\") as f:\n",
    "        dataset_J = json.load(f)\n",
    "        for i in dataset_J:\n",
    "            datum = dataset_J[i]\n",
    "            # if datum['Qcate'] == 'txt':\n",
    "            if use_num_samples == -1 or count < use_num_samples:\n",
    "                guid = datum['Guid']\n",
    "                qcate = datum['Qcate'] if 'Qcate' in datum else 'TBD'\n",
    "                Q = datum['Q'].replace('\"', \"\")\n",
    "                A = datum['A'][0].replace('\"', \"\")\n",
    "                A_list = [a.replace('\"', \"\") for a in datum['A']]\n",
    "                try: Keywords_A = datum['Keywords_A'].replace('\"', \"\")\n",
    "                except: Keywords_A = \"TBD\"\n",
    "                gold_facts = []\n",
    "                \n",
    "\n",
    "                for idx,fa in enumerate(datum['txt_posFacts_path']):\n",
    "                    # encode fa using a encoder function and save the path by creating a new key 'txt_posFacts_path'\n",
    "                    print('1')\n",
    "                    fa=fa.replace('/Users/aavi/Desktop/11-777-Project/Data/univlr_features','/home/ubuntu/Project/11-777-Project-aa/Data/univlr_features')\n",
    "                    gold_facts.append(fa)\n",
    "                self.instance_list.append((gold_facts, [], [], [], Q, A, Keywords_A, A_list, False, \"txt\", guid, qcate)) # do_filter_task, context\n",
    "                \n",
    "                count += 1\n",
    "            else: \n",
    "                  # if ('all' in Qcate) or datum['Qcate'] in Qcate:\n",
    "                  #     if use_num_samples == -1 or count < use_num_samples:\n",
    "                  #         guid = datum['Guid']\n",
    "                  #         qcate = datum['Qcate'] if 'Qcate' in datum else 'TBD'\n",
    "                  #         Q = datum['Q'].replace('\"', \"\")\n",
    "                  #         A = datum['A'][0].replace('\"', \"\")\n",
    "                  #         A_list = [a.replace('\"', \"\") for a in datum['A']]\n",
    "                  #         try: Keywords_A = datum['Keywords_A'].replace('\"', \"\")\n",
    "                  #         except: Keywords_A = \"TBD\"\n",
    "                  #         gold_feature = []\n",
    "            \n",
    "                  #         for idx,fa in enumerate(datum['img_posFacts_path']):\n",
    "                  #         # encode fa using a encoder function and save the path by creating a new key 'txt_posFacts_path'\n",
    "                  #           fa=fa.replace('/Users/aavi/Desktop/11-777-Project/Data/univlr_features','/home/ubuntu/Project/11-777-Project-aa/Data/univlr_features')\n",
    "                  #           gold_feature.append(fa)\n",
    "                  #         self.instance_list.append((gold_feature, [], [], [], Q, A, Keywords_A, A_list, False, \"img\", guid, qcate)) # do_filter_task, context )\n",
    "                  #         count += 1\n",
    "                  pass\n",
    "        print(\"Load {} instances from {} samples\".format(len(self.instance_list), count))\n",
    "\n",
    "\n",
    "\n",
    "    logging.debug('Done loading data.')\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.instance_list)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    gold_feature, [], [], [], Q, A, Keywords_A, A_list, _, type1, _, qcate= self.instance_list[idx]\n",
    "    while(len(gold_feature)<2):\n",
    "      gold_feature.append(gold_feature[0])\n",
    "    tokenized_data = self.tokenizer(\n",
    "        A,\n",
    "        return_tensors=\"pt\",\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=self.max_len)\n",
    "    tokens = tokenized_data.input_ids[0]\n",
    "    Q='Q: ' + Q + '\\nA:'\n",
    "    caption_len = tokenized_data.attention_mask[0].sum()\n",
    "\n",
    "      \n",
    "    return gold_feature, Q, tokens, caption_len,type1,A\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Transpose the batch (convert a list of tuples to a tuple of lists)\n",
    "    batch = list(zip(*batch))\n",
    "\n",
    "    # Process each component separately\n",
    "    visual_features = batch[0]  # Assuming the first element is visual features\n",
    "    questions = batch[1]  # Assuming the second element is questions\n",
    "    tokens = torch.stack(batch[2])  # Assuming the third element is tokens\n",
    "    caption_lengths = torch.tensor(batch[3])  # Assuming the fourth element is caption lengths\n",
    "    types = batch[4]  # Assuming the fifth element is data types (\"img\" or \"txt\")\n",
    "    answers = batch[5]\n",
    "    return visual_features, questions, tokens, caption_lengths, types, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'txt_posFacts_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/Project/Project_final/11-777-Project/fromage/fromage/check.ipynb Cell 3\u001b[0m line \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bavi/home/ubuntu/Project/Project_final/11-777-Project/fromage/fromage/check.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Data loading code\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bavi/home/ubuntu/Project/Project_final/11-777-Project/fromage/fromage/check.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(opt_version, use_fast\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bavi/home/ubuntu/Project/Project_final/11-777-Project/fromage/fromage/check.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m train_dataset \u001b[39m=\u001b[39m get_dataset( \u001b[39m'\u001b[39;49m\u001b[39md\u001b[39;49m\u001b[39m'\u001b[39;49m,[\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m], tokenizer)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bavi/home/ubuntu/Project/Project_final/11-777-Project/fromage/fromage/check.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# val_dataset = get_dataset( 'd',['val'], tokenizer)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bavi/home/ubuntu/Project/Project_final/11-777-Project/fromage/fromage/check.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# print(f'Training with {len(train_dataset)} examples and validating with {len(val_dataset)} examples.')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bavi/home/ubuntu/Project/Project_final/11-777-Project/fromage/fromage/check.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m train_sampler \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/home/ubuntu/Project/Project_final/11-777-Project/fromage/fromage/check.ipynb Cell 3\u001b[0m line \u001b[0;36mget_dataset\u001b[0;34m(args, split, tokenizer, precision)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bavi/home/ubuntu/Project/Project_final/11-777-Project/fromage/fromage/check.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_dataset\u001b[39m(args,split, tokenizer, precision: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mfp32\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dataset:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bavi/home/ubuntu/Project/Project_final/11-777-Project/fromage/fromage/check.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     dataset\u001b[39m=\u001b[39m CsvDataset(tokenizer, \u001b[39m'\u001b[39;49m\u001b[39m/home/ubuntu/Project/11-777-Project-aa/Data/WebQA_train_val_final.json\u001b[39;49m\u001b[39m'\u001b[39;49m,split\u001b[39m=\u001b[39;49msplit)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bavi/home/ubuntu/Project/Project_final/11-777-Project/fromage/fromage/check.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m dataset\n",
      "\u001b[1;32m/home/ubuntu/Project/Project_final/11-777-Project/fromage/fromage/check.ipynb Cell 3\u001b[0m line \u001b[0;36mCsvDataset.__init__\u001b[0;34m(self, tokenizer, dataset_json_path, use_num_samples, split, Qcate, max_len, sep, precision)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bavi/home/ubuntu/Project/Project_final/11-777-Project/fromage/fromage/check.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39mexcept\u001b[39;00m: Keywords_A \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mTBD\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bavi/home/ubuntu/Project/Project_final/11-777-Project/fromage/fromage/check.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m gold_facts \u001b[39m=\u001b[39m []\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bavi/home/ubuntu/Project/Project_final/11-777-Project/fromage/fromage/check.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39mfor\u001b[39;00m idx,fa \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(datum[\u001b[39m'\u001b[39;49m\u001b[39mtxt_posFacts_path\u001b[39;49m\u001b[39m'\u001b[39;49m]):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bavi/home/ubuntu/Project/Project_final/11-777-Project/fromage/fromage/check.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m     \u001b[39m# encode fa using a encoder function and save the path by creating a new key 'txt_posFacts_path'\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bavi/home/ubuntu/Project/Project_final/11-777-Project/fromage/fromage/check.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bavi/home/ubuntu/Project/Project_final/11-777-Project/fromage/fromage/check.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m     fa\u001b[39m=\u001b[39mfa\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m/Users/aavi/Desktop/11-777-Project/Data/univlr_features\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m/home/ubuntu/Project/11-777-Project-aa/Data/univlr_features\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'txt_posFacts_path'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "opt_version= 'facebook/opt-1.3b'\n",
    "# Data loading code\n",
    "tokenizer = AutoTokenizer.from_pretrained(opt_version, use_fast=False)\n",
    "train_dataset = get_dataset( 'd',['train'], tokenizer)\n",
    "# val_dataset = get_dataset( 'd',['val'], tokenizer)\n",
    "# print(f'Training with {len(train_dataset)} examples and validating with {len(val_dataset)} examples.')\n",
    "\n",
    "train_sampler = None\n",
    "val_sampler = None\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "train_dataset, batch_size=4,\n",
    "num_workers=4,collate_fn=collate_fn)\n",
    "# val_loader = torch.utils.data.DataLoader(\n",
    "# val_dataset, batch_size=2, shuffle=False,\n",
    "# num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['/home/ubuntu/Project/11-777-Project-aa/Data/univlr_features/d5bbc6d80dba11ecb1e81171463288e9_0.pkl', '/home/ubuntu/Project/11-777-Project-aa/Data/univlr_features/d5bbc6d80dba11ecb1e81171463288e9_1.pkl'], ['/home/ubuntu/Project/11-777-Project-aa/Data/univlr_features/d5bbc7640dba11ecb1e81171463288e9_0.pkl', '/home/ubuntu/Project/11-777-Project-aa/Data/univlr_features/d5bbc7640dba11ecb1e81171463288e9_1.pkl'], ['/home/ubuntu/Project/11-777-Project-aa/Data/univlr_features/d5bbc8cc0dba11ecb1e81171463288e9_0.pkl', '/home/ubuntu/Project/11-777-Project-aa/Data/univlr_features/d5bbc8cc0dba11ecb1e81171463288e9_1.pkl'], ['/home/ubuntu/Project/11-777-Project-aa/Data/univlr_features/d5bbc91c0dba11ecb1e81171463288e9_0.pkl', '/home/ubuntu/Project/11-777-Project-aa/Data/univlr_features/d5bbc91c0dba11ecb1e81171463288e9_1.pkl'])\n",
      "('Q: Are both the National Museum of the American Indian in Washington, D.C. and the Xanadu House in Kissimmee, Florida the same color?\\nA:', 'Q: Are the satellites on the Soviet space control/monitoring ship Kosmonavt Yuriy Gagarin always oriented in the same direction?\\nA:', 'Q: How many ground floor entrances are there to the Neumayer station in Antarctica?\\nA:', 'Q: What color are both domes on the Imam Husayn Shrine and the  Imam Ali Shrine?\\nA:')\n",
      "tensor([[    2,  9904,     6,   258,     5,   496,  4355,     9,     5,   470,\n",
      "          1362,    11,   663,     6,   211,     4,   347,     4,     8,     5,\n",
      "         38965,   625,   257,   446,    11, 22426,   757,  1794,   242,     6,\n",
      "          1261,    32],\n",
      "        [    2,   133, 16350,    15,     5,  8297,   980,   797,    73, 38575,\n",
      "           154,  3627,  8744,  5806,  1469,    90,   854,   710,  4911,   272,\n",
      "          1073, 15394,  2025,    75,   460, 28094,    11,     5,   276,  2698,\n",
      "             4,     1],\n",
      "        [    2,   970,    16,    65,  1255,  1929,  7266,     7,     5,  3864,\n",
      "           783, 19777,  1992,    11, 27593,     4,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1],\n",
      "        [    2, 20439,    16,     5,  3195,   341,    11,   258, 13567,   293,\n",
      "             9,     5, 30901,  9054, 27642, 35880,     8,     5, 30901,  4110,\n",
      "         35880,     4,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1]])\n",
      "torch.Size([4])\n",
      "('img', 'img', 'img', 'img')\n",
      "('Yes, both the National Museum of the American Indian in Washington, D.C. and the Xanadu House in Kissimmee, Florida are beige.', \"The satellites on the Soviet space control/monitoring ship Kosmonavt Yuriy Gagarin aren't always oriented in the same direction.\", 'There is one ground floor entrance to the Neumayer station in Antarctica.', 'Gold is the color used in both domes of the Imam Husayn Shrine and the Imam Ali Shrine.')\n"
     ]
    }
   ],
   "source": [
    "#generate samples for training\n",
    "for batch_idx, (visual_features, questions, tokens, caption_lengths, types,A) in enumerate(train_loader):\n",
    "    print(visual_features)\n",
    "    print(questions)\n",
    "    print(tokens)\n",
    "    print(caption_lengths.shape)\n",
    "    print(types)\n",
    "    print(A)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the next batch\n",
    "visual_features, questions, tokens, caption_lengths, types=next(iter(train_loader))\n",
    "# print(len(visual_features[0]))\n",
    "# print(len(questions))\n",
    "# print(tokens.shape)\n",
    "# print(caption_lengths.shape)\n",
    "# print(types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(visual_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 2048])\n",
      "torch.Size([100, 2048])\n",
      "torch.Size([2, 100, 4096])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import io\n",
    "\n",
    "def read_and_stack_features(batch_paths):\n",
    "    features_list = []\n",
    "\n",
    "    for path_pair in batch_paths:\n",
    "        feature1_path, feature2_path = path_pair\n",
    "\n",
    "        # Read features\n",
    "        with open(feature1_path, \"rb\") as f:\n",
    "            feature1 = pickle.load(f)\n",
    "        feature1 = feature1['fc1_features']\n",
    "        with open(feature2_path, \"rb\") as f:\n",
    "            feature2 = pickle.load(f)\n",
    "        feature2 = feature2['fc1_features']\n",
    "        print(feature1.shape)\n",
    "        # Assuming feature1 and feature2 are PyTorch tensors\n",
    "        # If not, you may need to convert them to tensors using torch.tensor()\n",
    "\n",
    "        # Stack features along a new dimension (concatenate along the last axis)\n",
    "        #stacked_features = torch.stack([feature1, feature2])\n",
    "        merged_feature = torch.cat((feature1, feature2), 1)\n",
    "\n",
    "        # Append the stacked features to the list\n",
    "        features_list.append(merged_feature)\n",
    "\n",
    "    # Stack the list of stacked features along a new dimension\n",
    "    batch_tensor = torch.stack(features_list)\n",
    "\n",
    "    return batch_tensor\n",
    "\n",
    "# Example usage:\n",
    "batch_paths = [\n",
    "    ('path/to/feature1/image1.png', 'path/to/feature2/image1.png'),\n",
    "    ('path/to/feature1/image2.png', 'path/to/feature2/image2.png'),\n",
    "    # Add more paths as needed\n",
    "]\n",
    "\n",
    "result = read_and_stack_features(visual_features)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 51200])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Input tensor with size (2, 100, 4096)\n",
    "input_tensor = torch.randn((2, 100, 4096))\n",
    "\n",
    "# Define a model with linear layers and reshaping\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.linear_layer = nn.Linear(4096, 2048)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply linear layer\n",
    "        x = self.linear_layer(x)\n",
    "\n",
    "        # Reshape the tensor to (2, 4, 2048)\n",
    "        x = x.view(x.size(0), 4, -1)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = CustomModel()\n",
    "\n",
    "# Forward pass\n",
    "output_tensor = model(input_tensor)\n",
    "\n",
    "# Print the size of the output tensor\n",
    "print(output_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "409600/(2048*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 2048])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Assuming you have input data x with size (2, 100, 4096)\n",
    "x = torch.rand((2, 100, 4096))\n",
    "\n",
    "# Define a simple neural network model\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomModel, self).__init__()\n",
    "\n",
    "        # Linear layer 1: Reduce the dimension from 4096 to 2048\n",
    "        self.linear1 = nn.Linear(100, 2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=x.view(x.size(0),-1,x.size(1))\n",
    "        x = self.linear1(x)\n",
    "        x = x.reshape(x.size(0), 4, -1)\n",
    "       \n",
    "        return x\n",
    "\n",
    "# Create an instance of the model\n",
    "model = CustomModel()\n",
    "\n",
    "# Forward pass to transform the input tensor\n",
    "output = model(x)\n",
    "\n",
    "# Print the size of the output tensor\n",
    "print(output.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fromage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
